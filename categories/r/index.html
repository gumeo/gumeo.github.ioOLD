<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>R &middot; Gumeo&#39;s Blog</title>

    <meta name="description" content="Machine Learning, Coding and Expat life">

    <meta name="generator" content="Hugo 0.30.2" />
    <meta name="twitter:card" content="summary">
    
    <meta name="twitter:title" content="R &middot; Gumeo&#39;s Blog">
    <meta name="twitter:description" content="Machine Learning, Coding and Expat life">

    <meta property="og:type" content="article">
    <meta property="og:title" content="R &middot; Gumeo&#39;s Blog">
    <meta property="og:description" content="Machine Learning, Coding and Expat life">

    <link href='//fonts.googleapis.com/css?family=Source+Sans+Pro:400,700|Oxygen:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/pure/0.6.0/pure-min.css">
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/pure/0.6.0/grids-responsive-min.css">

    <link rel="stylesheet" href="https://gumeo.github.io//css/all.min.css">
    <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css" rel="stylesheet">

    <link rel="alternate" type="application/rss+xml" title="Gumeo&#39;s Blog" href="https://gumeo.github.io//index.xml" />
    <link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
</head>
<body>


<div id="layout" class="pure-g">
    <div class="sidebar pure-u-1 pure-u-md-1-4">
    <div class="header">
        <hgroup>
            <h1 class="brand-title"><a href="https://gumeo.github.io/">Gumeo&#39;s Blog</a></h1>
            <h2 class="brand-tagline"> Machine Learning, Coding and Expat life </h2>
        </hgroup>

        <nav class="nav">
            <ul class="nav-list">
                
                <li class="nav-item">
                    <a class="pure-button" href="https://twitter.com/gumgumeo"><i class="fa fa-twitter"></i> Twitter</a>
                </li>
                
                
                <li class="nav-item">
                    <a class="pure-button" href="https://github.com/gumeo "><i class="fa fa-github-alt"></i> github</a>
                </li>
                
                
                <li class="nav-item">
                    <a class="pure-button" href="https://www.linkedin.com/in/gudmundur-einarsson-484406a7"><i class="fa fa-linkedin"></i> LinkedIn</a>
                </li>
                
            </ul>
        </nav>
    </div>
</div>


    <div class="content pure-u-1 pure-u-md-3-4">
        <div>
            
            <div class="posts">
                
                <h1 class="content-subhead">21 Dec 2017, 00:00</h1>
                <section class="post">
                    <header class="post-header">

                        <a href="https://gumeo.github.io/post/part-2-deep-learning-with-closures-in-r/" class="post-title">Part 2: Deep Learning with Closures in R</a>

                        <p class="post-meta">
                            
                                By <strong class="post-author">Gudmundur Einarsson</strong>
                            
                            
                                under 
                                
                                <a class="post-category post-category-Deep-Learning" href="https://gumeo.github.io//categories/deep-learning">Deep-Learning</a><a class="post-category post-category-Functional-Programming" href="https://gumeo.github.io//categories/functional-programming">Functional-Programming</a><a class="post-category post-category-Machine-Learning" href="https://gumeo.github.io//categories/machine-learning">Machine-Learning</a><a class="post-category post-category-R" href="https://gumeo.github.io//categories/r">R</a>
                            
                        </p>
                    </header>

                    <div class="post-description">
                        <div id="lets-go-on" class="section level1">
<h1>Let’s go on!</h1>
<p>If you just arrived, you can check out the first part <a href="https://gumeo.github.io/post/part-1-deep-learning-with-closures-in-r/">here</a>. The goal of this series is to demonstrate how compactly we can implement an MLP in a functional programming paradigm and how easy it becomes to extend/play around with it. This post is aimed at the R person that wants to get into deep learning or anyone curious about how these things are implemented. Another goal is to visualize what is happening in a neural network during training in hope to get a deeper understanding of what is going on. I posted <a href="https://imgur.com/EgcQgkh">this gif</a> on the subreddit MachineLearning and dataisbeautiful, I got some constructive criticism on the gif and below is a <em>better</em> version where I have sped it up and removed the flickering and enhanced the contrast.</p>
<center>
<blockquote class="imgur-embed-pub" lang="en" data-id="a/wIytV">
<a href="//imgur.com/wIytV"></a>
</blockquote>
<script async src="//s.imgur.com/min/embed.js" charset="utf-8"></script>
</center>
<p>So last time we got away with less than 50 lines of code to create a <a href="https://en.wikipedia.org/wiki/Factory_(object-oriented_programming)">factory</a> for generating fully connected layers. This included a closure for a matrix class, (for making the layer have a mutable state), and an implementation of a forward pass. But the forward pass is basically a wrapper for updating the internals of the layer, we need to implement a forward pass for the whole network, and the same goes for the backwards propagation. This requires creating a function factory that constructs the network environment. This environment needs to have functions that implement forward-, backwards passes and a function for training. The following is a demonstration of such a function:</p>
<pre class="r"><code># The following creates an MLP environment
# structNet is a vector defining the number of nodes in each layer, ignoring biases.
mlp &lt;- function(structNet, minibatchSize,activation, initPos =FALSE, initScale=100){
  num_layers &lt;- length(structNet)
  # Create the network
  layers &lt;- list()
  for(i in 1:num_layers){
    if(i == 1){ # Input layer
      layers[[i]] &lt;- Layer(activation, 
                           minibatchSize, 
                           c(structNet[1]+1,structNet[2]),
                           is_input=TRUE,
                           initPos = initPos,
                           initScale=initScale)
    }else if(i == length(structNet)){ # Output layer
      layers[[i]] &lt;- Layer(softmax, 
                           minibatchSize, 
                           c(structNet[num_layers],structNet[num_layers]),
                           is_output=TRUE,
                           initPos = initPos,
                           initScale=initScale)
    }else{ # Hidden layers
      layers[[i]] &lt;- Layer(activation, 
                           minibatchSize, 
                           c(structNet[i]+1,structNet[i+1]),
                           initPos = initPos,
                           initScale=initScale)
    }
  }
  
  forward_prop &lt;- function(dataBatch){
    # Add bias to the input
    layers[[1]]$Z$setter(cbind(dataBatch,rep(1,nrow(dataBatch))))
    for(i in 1:(num_layers-1)){
      layers[[i+1]]$S$setter(layers[[i]]$forward_propagate())
    }
    return(layers[[num_layers]]$forward_propagate())
  }
  
  backwards_prop &lt;- function(yhat,labels){
    layers[[num_layers]]$D$setter(t(yhat-labels))
    for(i in (num_layers-1):2){
      W_nobias &lt;- layers[[i]]$W$getter()
      W_nobias &lt;- W_nobias[1:(nrow(W_nobias)-1),]
      mat &lt;- layers[[i]]$Fp$getter()
      layers[[i]]$D$setter((W_nobias%*%layers[[i+1]]$D$getter())*mat)
    }
  }
  
  update_weights &lt;- function(eta){
    for(i in 1:(num_layers-1)){
      W_grad &lt;- -eta*t(layers[[i+1]]$D$getter()%*%layers[[i]]$Z$getter())
      layers[[i]]$W$setter(layers[[i]]$W$getter()+W_grad)
    }
  }
  
  # Labels here as dummy matrix
  train &lt;- function(trainData, trainLabels, num_epochs, eta, cvSplit = 0.3){
    # Code for preparing input
    for(i in 1:num_epochs){
      # Loop over epochs
      for(j in 1:numIter){
        # Loop over all minibatches
        # ...
        # Essential part for training
        preds &lt;- forward_prop(tDat)
        backwards_prop(preds,tLab)
        update_weights(eta = eta)
      }
    }
  }
  
  myEnv &lt;- list2env(list(network=layers,
                         forward_prop=forward_prop,
                         train = train))
  class(myEnv) &lt;- &#39;mlp&#39;
  return(myEnv)
}</code></pre>
<p>The <code>mlp</code> function above starts by constructing the network given the layer configuration that we desire. Most of the implementation of the <code>train</code> function is omitted to highlight the essential part. The shown part highlights what is needed for stochastic gradient descent. We need to <strong>forward propagate</strong> the data, then <strong>back propagate</strong> the error, and finally based on this error we can <strong>update the weights</strong>. This is the heart and soul of SGD. Instead of using the whole dataset to estimate the gradient, we subsample a <em>minibatch</em> and estimate the gradient as an average over these samples. Quite a lot of gradient descent based optimization is done as a full batch optimization, specifically when people first learn about it, because SGD just adds an extra layer of complication. SGD allows for online updates, as more data arrives, and it may be computationally more efficient. The only issue is of course the diffculties of scaling the gradient, i.e. choosing the best learning rate, which is also an issue for regular/full battch gradient descent.</p>
<p>So the training function is essentially broken up into 3 steps, these steps are abstracted from the training algorithm, so we could easily change the <code>update_weights</code> function to include a momentum term or other things we might want to try, without changing the <code>train</code> function.</p>
<p>Notice that we take the vector <code>structNet</code> as input. This design can be extended to take in another vector describing what kind of layers we want and build the network with these different layers. This would generalize the implementation quite significantly. This would of course also require us to change the definition of the <code>Layer</code> function, or implement new layer functions.</p>
</div>
<div id="is-deep-learning-simple" class="section level1">
<h1>Is deep learning simple?</h1>
<p>Now we have an <code>mlp</code> implementation in roughly 100 lines of code. I think this demonstrates in some way how <em>simple</em> an mlp really is, but also in contrast the difficulties needed to get to the modern implementations of deep learning. Modern implementations are much more general, where they usually implement a computation graph where all computations can be automatically differentiated. These are also made to target GPU hardware, where matrix-matrix and matrix-vector multiplication can be significantly accelerated. There is a package for R called <code>gpuR</code>, where one can do matrix calculations on the GPU. I might try that in the future, to see if I can speed this up significantly with minimal change of this code.</p>
</div>
<div id="how-to-use-this" class="section level1">
<h1>How to use this?</h1>
<p>First we need to define the activation function we want, the softmax function and a function to create a dummy encoding of factors variables in R. I have now included all of these in the package <code>minstr</code> available <a href="https://github.com/gumeo/mnistr">here</a>. So you do not need to define the yourself. Although you can of course define your own activation function as you wish. Take a look at the following to get an idea of how it is done.</p>
<pre class="r"><code># rectified linear unit activation
# activation functions need a boolean parameter for 
# calculating the derivative
reLU &lt;- function(X,deriv=FALSE){
  if(deriv==FALSE){
    X[X&lt;0] &lt;- 0 
    return(X)
  }else{
    X[X&lt;0] &lt;- 0
    X[X&gt;0] &lt;- 1
    return(X)
  }
}

# softmax for output layer
softmax &lt;- function(X){
  Z &lt;- rowSums(exp(X))
  X &lt;- exp(t(X))%*%diag(1/Z)
  return(t(X))
}

# Function to represent factor as dummy matrix
class.ind &lt;- function(cl) {
  Ik=diag(length(levels(cl)))
  x=Ik[as.numeric(cl),]
  dimnames(x) &lt;- list(names(cl), levels(cl))
  x
}</code></pre>
<p>Here is an example of how we can train a network with this. Feel free to play around with some of the parameters, such as the learning rate. If it is too big, nothing will be learned. The current configuration for the network works fine, but you can also try to squeeze it down or remove/add more hidden layers.</p>
<pre class="r"><code># Load mnist dataset, note that you need to download it first
# mnistr::download_mnist() # This saves to the current working directory
mnistr::load_mnist()

# Visualize random digit, just to get familiar with the data, in case this is the 
# first time you see mnist
randDig &lt;- sample(1:60000,1)
# Base R, notice that the data is in trainSet$x where each digit is stored as a row vector
image(matrix(trainSet$x[randDig,],28,28)[,c(28:1),drop=FALSE],main=paste(trainSet$y[randDig]),asp=1)
# ggplot function from the mnistr package, we can supply the data as a row vector
mnistr::ggDigit(trainSet$x[randDig,])

# Construct a network with two hidden layers (100 and 50 units) and rectified linear units
mnw &lt;- mlp(structNet = c(784,100,50,10), 
           minibatchSize = 100, 
           activation = reLU,
           initPos = TRUE,
           initScale = 100)

# Define the training set
trainX &lt;- trainSet$x
trainY &lt;- class.ind(factor(trainSet$y))

# Call train
mnw$train(trainData = trainX/256,
          trainLabels = trainY,
          num_epochs = 3,
          eta = 0.005, 
          cvSplit = 0.3)</code></pre>
</div>
<div id="no-hidden-layers" class="section level1">
<h1>No hidden layers</h1>
<p>I wanted to visualize the training when there were no hidden layers. The weights in the gif below correspond to the training over one epoch, where the weights according to the digits are aligned like the matrix below: <span class="math display">\[
  \begin{bmatrix}
    2 &amp; 5 &amp; 8\\
    1 &amp; 4 &amp; 7\\
    0 &amp; 3 &amp; 6
  \end{bmatrix}
\]</span></p>
<center>
<blockquote class="imgur-embed-pub" lang="en" data-id="a/s29MY">
<a href="//imgur.com/s29MY"></a>
</blockquote>
<script async src="//s.imgur.com/min/embed.js" charset="utf-8"></script>
</center>
<p>You can clearly see that the weights resemble templates that evolve to capture more and more of the variation present for each digit. The template also counts for making the patch dissimilar to the digits that it is not trying to match. This simple linear approach achieves a little more than 90% accuracy, which is amazing for how simple the method is. But still, for reading digits on checks, having every tenth read wrong is far from acceptable.</p>
</div>
<div id="decomposing-the-network-with-magrittr" class="section level1">
<h1>Decomposing the network with magrittr</h1>
<p>Now, one of the main things I wanted to achieve with this implementation was to decompose the network into it’s individual components, to demonstrate how simple it truly is. Now I can in a simple manner compose functions that I have trained in order to produce some output. The <code>magrittr</code> is perfect for this, because then we do not need to nest inside parenthesis the composition of functions, we can just pipe the input through all the layers.</p>
<pre class="r"><code># Needed for pipes
library(magrittr)

# Get the weight matricies from the network we trained above
w1 &lt;- mnw$network[[1]]$W$getter()
w2 &lt;- mnw$network[[2]]$W$getter()
w3 &lt;- mnw$network[[3]]$W$getter()

set.seed(12345)
randDig &lt;- sample(1:60000,1) # sample random digit
testDigit &lt;- matrix(trainSet$x[randDig,]/256,nrow=1,ncol=28*28)
# Check label, it is a 2 for this seed
trainSet$y[randDig]
mnistr::ggDigit(testDigit)

# Add the bias term
testDigit &lt;- cbind(testDigit,matrix(1,1,1))

# We are passing a single instance
simple_softmax &lt;- function(X){
  Z &lt;- sum(exp(X))
  X &lt;- exp(X)/Z
  return(X)
}

# Now pipe it throught the network:
# Multiply by weights, use activation and then add 1 for bias
testDigit %&gt;% 
  multiply_by_matrix(y=w1) %&gt;% reLU %&gt;% cbind(.,matrix(1,1,1)) %&gt;% 
  multiply_by_matrix(y=w2) %&gt;% reLU %&gt;% cbind(.,matrix(1,1,1)) %&gt;%
  multiply_by_matrix(y=w3) %&gt;% simple_softmax %&gt;% barplot</code></pre>
<p>The network I trained is 99.6% certain that this is a two! The <code>magrittr</code> package allows us to write out the calculations happening inside the MLP in a very understandable way. Note that some networks used today have more than 50 layers, then this is not so useful anymore. In the case of 50plus layers we need other tricks to train it, e.g. skip connections and other optimizers.</p>
</div>
<div id="code-for-generating-the-gifs" class="section level1">
<h1>Code for generating the gifs</h1>
<p>If you want to create you own gifs, it is probably easiest to define your own <code>mlp</code> function, just look at the <code>mnistr</code> implementation. Then add something similar to the following where the main update is happening. You could of course also save the weights to a matrix outside the environment using the <code>&lt;&lt;-</code> scope assignment operator.</p>
<pre class="r"><code>        # Where we do the SGD steps in the train function part of mlp
        preds &lt;- forward_prop(tDat)
        backwards_prop(preds,tLab)
        update_weights(eta = eta)
        # The following is code for generating plots
        layerMat &lt;- layers[[1]]$W$getter()
        # Removw bias
        layerMat &lt;- layerMat[-nrow(layerMat),]
        # Set the number of columns and rows for the image
        nr &lt;- 10
        nc &lt;- 10
        weightsIm &lt;- matrix(0,28*nr,28*nc)
        w &lt;- 1
        # Fill the image with the relevant weights
        for(i in 1:nr){
          for(m in 1:nc){
            weightsIm[((i-1)*28+1):(i*28),((m-1)*28+1):(m*28)] &lt;- matrix(layerMat[,w],28,28)[,c(28:1),drop=FALSE]
            w &lt;- w + 1
          }
        }
        # Save for every fourth minibatch
        if(j%%4 == 1){
          data &lt;- c(weightsIm)
          # Standardize to remove flickering
          nData &lt;- qnorm(seq(0,1,len=28*28*nr*nc))[rank(data)]
          nIm &lt;- matrix(nData,28*nr,28*nc)
          png(paste0(&#39;./plots/&#39;,&#39;reLU&#39;,sprintf(&quot;%08d&quot;, k),&#39;.png&#39;),width=28*nc*2,height=28*nr*2)
          # Remove margins so it looks nicer
          par(mar = rep(0, 4))
          image(nIm,asp=1,col=gray(seq(0,1,by=0.005),1),axes=FALSE)
          dev.off()
          k &lt;- k+1
        }</code></pre>
</div>
<div id="next-post" class="section level1">
<h1>Next post</h1>
<p>For the next post I plan to compare different activation functions and implement dropout for the network. It will probably be the final post until I get new ideas for stuff to implement.</p>
<a href="https://twitter.com/share?ref_src=twsrc%5Etfw" class="twitter-share-button" data-size="large" data-text="Learning Deep Learning in R" data-url="https://gumeo.github.io/post/part-2-deep-learning-with-closures-in-r/" data-via="gumgumeo" data-hashtags="#rstats #machinelearning #deeplearning" data-lang="en" data-show-count="false">Tweet</a>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</div>

                    </div>
                </section>
                
                <h1 class="content-subhead">14 Dec 2017, 00:00</h1>
                <section class="post">
                    <header class="post-header">

                        <a href="https://gumeo.github.io/post/part-1-deep-learning-with-closures-in-r/" class="post-title">Part 1: Deep Learning with Closures in R</a>

                        <p class="post-meta">
                            
                                By <strong class="post-author">Gudmundur Einarsson</strong>
                            
                            
                                under 
                                
                                <a class="post-category post-category-Machine-Learning" href="https://gumeo.github.io//categories/machine-learning">Machine-Learning</a><a class="post-category post-category-Deep-Learning" href="https://gumeo.github.io//categories/deep-learning">Deep-Learning</a><a class="post-category post-category-R" href="https://gumeo.github.io//categories/r">R</a><a class="post-category post-category-Functional-Programming" href="https://gumeo.github.io//categories/functional-programming">Functional-Programming</a>
                            
                        </p>
                    </header>

                    <div class="post-description">
                        <div id="start-of-a-small-series" class="section level1">
<h1>Start of a small series</h1>
<p>The gif below is the evolution of the weights from a neural network trained on the mnist dataset. Mnist is a dataset of handwritten digits, and is kind of the <strong>hello world/FizzBuzz</strong> of machine learning. It is maybe not a very challenging data set, but you can learn a lot from it. This is a 10 by 10 grid of images, where each individual small patch represents weights going to a single neuron in the first hidden layer of the network. After I saw the content by Grant Sanderson, I wanted to inspect by myself what the network is actually learning. This series is going to outline this development, with an angle towards functional programming.</p>
<center>
<blockquote class="imgur-embed-pub" lang="en" data-id="EgcQgkh">
<a href="//imgur.com/EgcQgkh"></a>
</blockquote>
<script async src="//s.imgur.com/min/embed.js" charset="utf-8"></script>
</center>
<p>So I received the <a href="http://www.deeplearningbook.org/">Deep Learning book</a> a little more than a month ago, and I have had time to read parts I and II. I think that overall the authors successfully explain and condense a lot of research into something digestable. The reason why I use the word condense is because how much information the book contains. The bibliography is 55 pages, so I almost feel that the book should be called <em>Introduction to Deep Learning Research</em>, because it is a gateway to so much good material. Another fascinating thing about this book is the discussion it contains. The authors are quite upfront about some criticisms on deep learning and discuss them to a great extent. All in all I look forward to finish reading the book.</p>
<p>I have fiddled around with deep learning since I took a summer-school on it in 2014, where <a href="https://twitter.com/hugo_larochelle?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor">Hugo Larochelle</a> was giving lectures and he joined us for an epic sword fighting competition/LARP session in the countryside of Denmark. I have followed the evolution of deep learning since, and what has been most amazing is by how much the barrier of entry has been lowered. The current software frameworks make it so easy to get started. After reading the DL book I found a strong inner urge to implement some of these things myself. I think that a way to get a better understanding of programming concepts, algorithms and data structures, is just to go for it and implement it. Also talking about data-structures, deep learning is also becoming a part of that:</p>
<center>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Jeff Dean and co at GOOG just released a paper showing how machine-learned indexes can replace B-Trees, Hash Indexes, and Bloom Filters. Execute 3x faster than B-Trees, 10-100x less space. Executes on GPU, which are getting faster unlike CPU. Amazing.  <a href="https://t.co/PPVkrFVKXg">https://t.co/PPVkrFVKXg</a></p>&mdash; Nick Schrock (@schrockn) <a href="https://twitter.com/schrockn/status/940037656494317568?ref_src=twsrc%5Etfw">December 11, 2017</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

</center>
<p>After seeing the videos made by <a href="http://www.3blue1brown.com/">Grant Sanderson</a> on deep learning I decided it was time to implement the basics by myself. I wanted to completely understand back-propagation, how it really is a dynamic programming algorithm where we actually do some smart book-keeping and reuse computations. That is one of the major <em>tricks</em> that makes this algorithm work.</p>
<div id="not-another-framework" class="section level2">
<h2>Not another framework</h2>
<p>But of course implementing a fully fledged DL framework is a feat one should not do, unless you have some very specific reason for it. Many frameworks have been created, and doing this alone <strong>to learn</strong> should not have that goal in mind. I wanted to make something that would be easily extendable, I also wanted to do it in R (because I really like R), I also made a package at some point called <a href="https://github.com/gumeo/mnistr">mnistr</a> where I wanted to add some fun examples with neural networks on. When I’m done with this series I’ll add the implementation to the package and submit it to CRAN.</p>
</div>
<div id="closures" class="section level2">
<h2>Closures</h2>
<p>The final reason I had for doing this relates to closures. So deep learning, or deep neural networks, are in it’s essance just functions, or rather compositions of functions. These individual functions are usually not that complicated, what makes them complicated is what they learn from complicated data, and how these individual simple things together make something complicated. We do not completely understand what they learn or do. If I can make a representation of a multi-layer percepteron (<em>which is a fully connected neural network</em>) in a functional programming paradigm, then it might be easier to understand it for others and myself. I will hopefully be able to disentanlge the networks into inidvidual functions and using the <a href="https://cran.r-project.org/web/packages/magrittr/index.html">magrittr</a> package to do the function composition in a more obvious manner, which demonstrates that the individual pieces of a neural network are not so complicated, and the final composition does not need to be so complicated either.</p>
</div>
<div id="but-wait-a-minute-what-are-closures" class="section level2">
<h2>But wait a minute? What are closures?</h2>
<p>So the <a href="https://en.wikipedia.org/wiki/Closure_(computer_programming)">wikipedia article</a> gives it a pretty good treatment</p>
<blockquote>
<p>In programming languages, closures (also lexical closures or function closures) are techniques for implementing lexically scoped name binding in languages with first-class functions. Operationally, a closure is a record storing a function[a] together with an environment:[1] a mapping associating each free variable of the function (variables that are used locally, but defined in an enclosing scope) with the value or reference to which the name was bound when the closure was created.[b] A closure—unlike a plain function—allows the function to access those captured variables through the closure’s copies of their values or references, even when the function is invoked outside their scope</p>
</blockquote>
<p>This sounds a lot like object oriented programming, but when the <em>objects</em> that we are working with, are naturally functions, then this works very nicely. But let’s look at a simple example!</p>
<pre class="r"><code>newBankAccount &lt;- function(){
  myMoney &lt;- 0
  putMonyInTheBank &lt;- function(amount){
    myMoney &lt;&lt;- myMoney + amount
  }
  howMuchDoIOwn &lt;- function(){
    print(paste(&#39;You have:&#39;,myMoney,&#39;bitcoins!&#39;))
  }
  return(list2env(list(putMonyInTheBank=putMonyInTheBank,
                       howMuchDoIOwn=howMuchDoIOwn)))
}</code></pre>
<p>Now I can use this to create a <em>bank account function</em></p>
<pre class="r"><code>&gt; myAccount &lt;- newBankAccount()
&gt; myAccount$howMuchDoIOwn()
[1] &quot;You have: 0 bitcoins!&quot;
&gt; myAccount$putMonyInTheBank(200)
&gt; myAccount$howMuchDoIOwn()
[1] &quot;You have: 200 bitcoins!&quot;
&gt; copyAccount &lt;- myAccount
&gt; copyAccount$howMuchDoIOwn()
[1] &quot;You have: 200 bitcoins!&quot;
&gt; copyAccount$putMonyInTheBank(300)
&gt; copyAccount$howMuchDoIOwn()
[1] &quot;You have: 500 bitcoins!&quot;
&gt; myAccount$howMuchDoIOwn() # Ahh snap, I also received bitcoins!
[1] &quot;You have: 500 bitcoins!&quot;</code></pre>
<p>So compared to <em>normal</em> functions, now we have a function (<em>actually an environment</em>) with a mutable state. Now we can have side-effects when we call the functions from the environment. But if you look at the calls I made above, you may have noticed that when I copied the account, added to the copied account, the original account also had an increased amount in it. So the copied account didn’t get the data copied, only the references. So if you make multiple bank accounts, initialize each seperately, don’t initialize one prototype and copy it to all the users. Otherwise all the users end up with one big shared account!</p>
<p>So be careful, and if you really want to copy environments look at <a href="https://stackoverflow.com/questions/9965577/r-copy-move-one-environment-to-another">this SO post</a>. If you want to learn more about environments and the functional programming side of R, <a href="http://adv-r.had.co.nz/">advanced R</a> by Hadley Wickham is a great starting point, you might also want to check out <a href="http://www.lemnica.com/esotericR/Introducing-Closures/">this blogpost</a>. Also if you have coded in javascript, you might be familiar with the issue of copying closures there. And btw, I have no bitcoins :(</p>
<p>Another important thing that you might have noticed is the assignment operators. If you are not familiar with R, <code>&lt;-</code> is pretty much the same as <code>=</code>, they are kind of used interchangably, but they have a very subtle difference that you can read about <a href="https://stackoverflow.com/questions/1741820/assignment-operators-in-r-and">here</a>. The weird assignment operator that you noticed was probably the <code>&lt;&lt;-</code>, the scoping assignment. This is the whole trick about closures, best said <a href="https://stackoverflow.com/questions/2628621/how-do-you-use-scoping-assignment-in-r?rq=1">here</a>:</p>
<blockquote>
<p>A closure is a function written by another function. Closures are so called because they enclose the environment of the parent function, and can access all variables and parameters in that function.</p>
</blockquote>
<p>The scoping operator creates the reference needed, such that the returned function encloses the environment of the caller. This is why closures are sometimes called poor man’s objects. We are basically emulating the creation of public and private members in some sense, but without the overhead of object oriented structured code. This is an essential part in making the implementation look cleaner and more transparent. The code that you write is more to the point of solving the task as hand, rather the adhearing to a structure of a particular paradigm.</p>
</div>
<div id="too-the-point-on-deep-learning-again" class="section level2">
<h2>Too the point on deep learning again!</h2>
<p>For the structure of the implementation I was very inspired by <a href="http://briandolhansky.com/blog/2014/10/30/artificial-neural-networks-matrix-form-part-5">this post</a> by Brian Dolhansky, where he implements an MLP in python.</p>
<p>This structure completely encapsulates what is needed in a layer and how things are connected. Instead of creating a class, I am going to make a closure. So when I say a <strong>layer</strong>, I mean the activations from the previous layers and the outgoing weights. So the only layer that doesn’t have, or doesn’t need weights, is the output layer.</p>
<p>This closure is therefore a function, or has some functions, which makes sense for a layer in a neural network, which is essentially a function in the mathematical sense. But before we get to the layer, we need what essentially creates the closure, a building block for a matrix:</p>
<pre class="r"><code>matrixInLayer &lt;- function(init = FALSE, rS, cS, initPos = FALSE, initScale = 100){
  intMat &lt;- matrix(0, nrow=rS, ncol=cS)
  if(init == TRUE){
    intMat &lt;- matrix(rnorm(rS*cS)/initScale,nrow = rS,ncol = cS)
    if(initPos == TRUE){
      intMat &lt;- abs(intMat)
    }
  }
  getter &lt;- function(){
    return(intMat)
  }
  setter &lt;- function(nMat){
    intMat &lt;&lt;- nMat
  }
  return(list2env(list(setter = setter, getter=getter)))
}</code></pre>
<p>We parameterize this function such that we can account for different initialization strategies in the weights, but we can use this to create all the needed matricies in a layer. The layer closure is then just something that encapsulates the internal state of the network, with placeholders for all the data needed to do a forward and backwards pass. The essential trick to make this work is of course the scope assignment in the <code>setter</code> function. The fully connected layer can now be implemented as:</p>
<pre class="r"><code>Layer &lt;- function(activation, minibatchSize, sizeP, is_input=FALSE, is_output=FALSE, initPos, initScale){
  # Matrix holding the output values
  Z &lt;- matrixInLayer(FALSE,minibatchSize,sizeP[1])
  # Outgoing Weights
  W &lt;- matrixInLayer(TRUE,sizeP[1],sizeP[2],initPos=initPos, initScale=initScale)
  # Input to this layer
  S &lt;- matrixInLayer(FALSE,minibatchSize,sizeP[1])
  # Deltas for this layer
  D &lt;- matrixInLayer(FALSE,minibatchSize,sizeP[1])
  # Matrix holding derivatives of the activation function
  Fp &lt;- matrixInLayer(FALSE,sizeP[1],minibatchSize)
  # Propagate minibatch through this layer
  forward_propagate &lt;- function(){
    if(is_input == TRUE){
      return(Z$getter()%*%W$getter())
    }
    Z$setter(activation(S$getter()))
    if(is_output == TRUE){
      return(Z$getter())
    }else{
      # Add bias for the hidden layer
      Z$setter(cbind(Z$getter(),rep(1,nrow(Z$getter()))))
      # Calculate the derivative
      Fp$setter(t(activation(S$getter(),deriv = TRUE))) 
      return(Z$getter()%*%W$getter())
    }
  }
  # Return a list of these functions
  myEnv &lt;- list2env(list(forward_propagate=forward_propagate, S = S,
                         D = D, Fp = Fp, W = W, Z = Z))
  class(myEnv) &lt;- &#39;Layer&#39;
  return(myEnv)
}</code></pre>
<p>The layer function includes all the things needed for doing the book-keeping of the calculations for backpropagation. With very little extra code we have the ability to have arbitrary minibatch sizes and arbitrary activation functions. The activation function just needs and extra boolean parameter to determine whether we are evaluating the function or calculating the deivative. (<em>I’ll go in more detail in the next post about what a minibatch is when I cover stocastic gradient descent</em>). We can protype these activation functions on the fly, because R is a functional language. So in less than 50 lines of code we have already created the heart of a multilayer percepteron, with some generalizability. So what does this layer do? It takes input from the activations of the neurons in the previous layer as a vector, multiplies it with a matrix and element-wise applies the activation. In essance it is this: <span class="math display">\[
  \sigma(\mathbf{W}\cdot \mathbf{x})
\]</span> where <span class="math inline">\(\mathbf{W}\)</span> is the weight matrix, <span class="math inline">\(\mathbf{x}\)</span> is the input and <span class="math inline">\(\sigma\)</span> is the activation function. The problem of deep learning is then to find good parameters for the weights <span class="math inline">\(\mathbf{W}\)</span> when these functions are composed.</p>
</div>
<div id="next-post" class="section level2">
<h2>Next post</h2>
<p>This ended up being a lot longer than I anticipated, but for the next post I aim at finishing the implementation for the MLP, going through backpropagation and simple training on mnist. For the last post the goal is to show how something like dropout can very easily be incorporated in this implementation and how we can disentangle a network using the magrittr package. Implementing other kinds of layers and different optimization strategies is also on the drawing board.</p>
<p>If you like this post I would greatly appreciate a tweet, my twitter handle is <span class="citation">@gumgumeo</span> :)</p>
<a href="https://twitter.com/share?ref_src=twsrc%5Etfw" class="twitter-share-button" data-size="large" data-text="Learning Deep Learning in R" data-url="https://gumeo.github.io/post/part-1-deep-learning-with-closures-in-r/" data-via="gumgumeo" data-hashtags="#rstats #machinelearning #deeplearning" data-lang="en" data-show-count="false">Tweet</a>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</div>
</div>

                    </div>
                </section>
                
                <h1 class="content-subhead">08 Nov 2017, 00:00</h1>
                <section class="post">
                    <header class="post-header">

                        <a href="https://gumeo.github.io/post/got-some-new-books/" class="post-title">Got some new books!</a>

                        <p class="post-meta">
                            
                                By <strong class="post-author">Gudmundur Einarsson</strong>
                            
                            
                                under 
                                
                                <a class="post-category post-category-Machine-Learning" href="https://gumeo.github.io//categories/machine-learning">Machine-Learning</a><a class="post-category post-category-R" href="https://gumeo.github.io//categories/r">R</a><a class="post-category post-category-Sparse" href="https://gumeo.github.io//categories/sparse">Sparse</a>
                            
                        </p>
                    </header>

                    <div class="post-description">
                        <p>I have been waiting to read these for a while now!</p>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Let the reading commence <a href="https://twitter.com/hashtag/rstats?src=hash&amp;ref_src=twsrc%5Etfw">#rstats</a> <a href="https://twitter.com/hashtag/MachineLearning?src=hash&amp;ref_src=twsrc%5Etfw">#MachineLearning</a> <a href="https://t.co/Lqx2IcTENI">pic.twitter.com/Lqx2IcTENI</a></p>&mdash; Gudmundur Einarsson (@gumgumeo) <a href="https://twitter.com/gumgumeo/status/928251371865956354?ref_src=twsrc%5Etfw">November 8, 2017</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>


                    </div>
                </section>
                
                <h1 class="content-subhead">07 Nov 2017, 16:00</h1>
                <section class="post">
                    <header class="post-header">

                        <a href="https://gumeo.github.io/post/2015-07-23-r-rmarkdown/" class="post-title">My first post</a>

                        <p class="post-meta">
                            
                                By <strong class="post-author">Gudmundur Einarsson</strong>
                            
                            
                                under 
                                
                                <a class="post-category post-category-R" href="https://gumeo.github.io//categories/r">R</a><a class="post-category post-category-Machine-Learning" href="https://gumeo.github.io//categories/machine-learning">Machine-Learning</a>
                            
                        </p>
                    </header>

                    <div class="post-description">
                        <div id="starting-a-blog" class="section level1">
<h1>Starting a blog</h1>
<p>The plan is to try to do weekly to monthly blogposts about something interesting I learn. I might also write up technical details about setting something up as a reference for my future self.</p>
<p>I chose to use the <a href="https://bookdown.org/yihui/blogdown/">blogdown</a> package to manage this blog, with the hugo theme <a href="http://dplesca.github.io/purehugo/"><code>purehugo</code></a>. The main reason I chose this is because I think it scales well for mobile devices and it looks clean and minimal. I will probably add an about section and some other stuff/fluff later.</p>
<p>This is not the first time that I start a blog, I also have my own <a href="http://www.imm.dtu.dk/~guei/">academic page/blog</a>. I just don’t post there regularly. Removing some of the overhead, by managing the content/posts as Rmarkdown documents, will hopefully make me more productive as a blogger.</p>
</div>

                    </div>
                </section>
                
            </div>
            

            <div class="footer">
    <div class="pure-menu pure-menu-horizontal pure-menu-open">
        <ul>
            <li>Powered by <a class="hugo" href="https://gohugo.io/" target="_blank">hugo</a></li>
        </ul>
    </div>
</div>
<script src="https://gumeo.github.io//js/all.min.js"></script>
<script src="//yihui.name/js/math-code.js"></script>
<script async
src="//cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/r.min.js"></script>

<script>
hljs.configure({languages: []});
hljs.initHighlightingOnLoad();
</script>
        </div>
    </div>
</div>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-110075286-1', 'auto');
ga('send', 'pageview');

</script>

</body>
</html>
